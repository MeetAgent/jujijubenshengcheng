#!/usr/bin/env python3
"""
FastAPI æœåŠ¡ï¼šå‰§é›†å‰§æœ¬ç”Ÿæˆæµæ°´çº¿ Web API
æ”¯æŒä¸ run_0_1_to_0_8.py ç›¸åŒçš„æ‰€æœ‰åŠŸèƒ½å’Œæ‰§è¡Œæ¨¡å¼
"""

import os
import time
import json
import asyncio
from typing import Dict, List, Any, Optional, Literal
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from enum import Enum

from new_pipeline.core import PipelineConfig, PipelineUtils
from new_pipeline.core.report_generator import PipelineReportGenerator
from new_pipeline.steps.step0_1_asr import Step0_1ASR
from new_pipeline.steps.step0_2_clue_extraction import Step0_2ClueExtraction
from new_pipeline.steps.step0_3_global_alignment_llm import Step0_3GlobalAlignmentLLM
from new_pipeline.steps.step0_4_speaker_calibration import Step0_4SpeakerCalibration
from new_pipeline.steps.step0_5_integrated_analysis import Step0_5IntegratedAnalysis
from new_pipeline.steps.step0_6_plot_extraction import Step0_6PlotExtraction
from new_pipeline.steps.step0_7_script_writing import Step0_7ScriptWriting
from new_pipeline.steps.step0_8_final_script import Step0_8FinalScript

from new_pipeline.steps.commont_log import log

from dotenv import load_dotenv
load_dotenv()


class ExecutionMode(str, Enum):
    """æ‰§è¡Œæ¨¡å¼æšä¸¾"""
    RERUN = "rerun"  # ä»å¤´å¼€å§‹é‡æ–°æ‰§è¡Œ
    RESUME = "resume"  # ä»æŒ‡å®šæ­¥éª¤å¼€å§‹ï¼ˆè·³è¿‡å·²å®Œæˆçš„æ­¥éª¤ï¼‰
    FORCE_RERUN = "force_rerun"  # å¼ºåˆ¶é‡è·‘æŒ‡å®šæ­¥éª¤
    FIX_INVALID = "fix_invalid"  # åªä¿®å¤æ— æ•ˆæ–‡ä»¶


class PipelineStats:
    """æµæ°´çº¿ç»Ÿè®¡ä¿¡æ¯"""
    def __init__(self):
        self.step_stats: Dict[str, Dict[str, Any]] = {}
        self.total_start_time = time.time()
    
    def start_step(self, step_name: str):
        """å¼€å§‹æ­¥éª¤è®¡æ—¶"""
        self.step_stats[step_name] = {
            "start_time": time.time(),
            "end_time": None,
            "duration": None,
            "tokens_used": 0,
            "status": "running"
        }
    
    def end_step(self, step_name: str, tokens_used: int = 0, status: str = "completed"):
        """ç»“æŸæ­¥éª¤è®¡æ—¶"""
        if step_name in self.step_stats:
            self.step_stats[step_name]["end_time"] = time.time()
            self.step_stats[step_name]["duration"] = self.step_stats[step_name]["end_time"] - self.step_stats[step_name]["start_time"]
            self.step_stats[step_name]["tokens_used"] = tokens_used
            self.step_stats[step_name]["status"] = status
    
    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        log.info("\n" + "="*60)
        log.info("ğŸ“Š æµæ°´çº¿æ‰§è¡Œç»Ÿè®¡")
        log.info("="*60)
        
        total_duration = time.time() - self.total_start_time
        total_tokens = sum(stats.get("tokens_used", 0) for stats in self.step_stats.values())
        
        log.info(f"æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
        log.info(f"æ€»Tokenæ¶ˆè€—: {total_tokens:,}")
        log.info("")
        
        for step_name, stats in self.step_stats.items():
            duration = stats.get("duration", 0)
            tokens = stats.get("tokens_used", 0)
            status = stats.get("status", "unknown")
            status_icon = "âœ…" if status == "completed" else "âŒ" if status == "failed" else "â³"
            
            log.info(f"{status_icon} {step_name}: {duration:.2f}ç§’, {tokens:,} tokens")
        
        log.info("="*60)


def check_file_integrity(config: PipelineConfig, step_name: str) -> bool:
    """æ£€æŸ¥æ­¥éª¤è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´æ€§"""
    output_root = config.project_root
    episodes = PipelineUtils.get_episode_list(output_root)
    
    # å®šä¹‰æ¯ä¸ªæ­¥éª¤çš„é¢„æœŸè¾“å‡ºæ–‡ä»¶
    expected_files = {
        "0.1": ["0_1_timed_dialogue.srt"],
        "0.2": ["0_2_clues.json"],
        "0.3": ["global_character_graph_llm.json"],
        "0.4": ["0_4_calibrated_dialogue.txt"],
        "0.5": ["0_5_dialogue_turns.json"],
        "0.6": ["0_6_script_flow.json", "0_6_script_draft.md"],
        "0.7": ["0_7_script.stmf", "0_7_script_analysis.json"],
        # 0.8 ä¸ºé›†åˆçº§åˆ«æ±‡æ€»äº§ç‰©ï¼ˆå­˜æ”¾åœ¨é›†åˆæ ¹ç›®å½• output_root ä¸‹ï¼‰
        "0.8": ["0_8_complete_screenplay.fountain", "0_8_complete_screenplay.fdx"]
    }
    
    if step_name not in expected_files:
        log.info(f"âš ï¸ æœªçŸ¥æ­¥éª¤: {step_name}")
        return True
    
    required_files = expected_files[step_name]
    missing_files = []
    
    for episode_id in episodes:
        for filename in required_files:
            if step_name == "0.3":  # å…¨å±€æ–‡ä»¶ï¼ˆåœ¨ global/ ä¸‹ï¼‰
                file_path = os.path.join(output_root, "global", filename)
            elif step_name == "0.8":  # å…¨å±€åˆå¹¶æ–‡ä»¶ï¼ˆé›†åˆæ ¹ç›®å½•ä¸‹ï¼‰
                file_path = os.path.join(output_root, filename)
            else:  # å‰§é›†æ–‡ä»¶
                file_path = os.path.join(output_root, episode_id, filename)
            
            if not os.path.exists(file_path):
                missing_files.append(f"{episode_id}/{filename}")
            else:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–æŸå
                try:
                    if filename.endswith('.json'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            json.load(f)
                    elif os.path.getsize(file_path) == 0:
                        missing_files.append(f"{episode_id}/{filename} (ç©ºæ–‡ä»¶)")
                except (json.JSONDecodeError, Exception) as e:
                    missing_files.append(f"{episode_id}/{filename} (æŸå: {e})")
    
    if missing_files:
        log.info(f"âŒ {step_name} æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥:")
        for missing in missing_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            log.info(f"   ç¼ºå°‘: {missing}")
        if len(missing_files) > 10:
            log.info(f"   ... è¿˜æœ‰ {len(missing_files) - 10} ä¸ªæ–‡ä»¶")
        return False
    
    log.info(f"âœ… {step_name} æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
    return True


def _fail(step: str, result) -> bool:
    """åˆ¤æ–­æ­¥éª¤æ˜¯å¦å¤±è´¥"""
    status = (result or {}).get("status")
    if step in ("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8"):
        return status != "completed" and status != "already_exists"
    return status != "success" and status != "already_exists"


def run_step(step_name: str, step_class, config: PipelineConfig, stats: PipelineStats) -> tuple[bool, Any, Any]:
    """è¿è¡Œå•ä¸ªæ­¥éª¤ï¼Œè¿”å› (æˆåŠŸçŠ¶æ€, æ­¥éª¤å®ä¾‹, ç»“æœ)"""
    log.info(f"\n{step_name}: {step_class.__name__} â€¦")
    stats.start_step(step_name)
    
    try:
        step_instance = step_class(config)
        result = step_instance.run()
        
        # æ£€æŸ¥æ­¥éª¤æ˜¯å¦æˆåŠŸ
        if _fail(step_name, result):
            log.info(f"âŒ {step_name} å¤±è´¥: {result}")
            stats.end_step(step_name, status="failed")
            return False, step_instance, result
        
        # æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
        if not check_file_integrity(config, step_name):
            log.info(f"âŒ {step_name} æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")
            stats.end_step(step_name, status="failed")
            return False, step_instance, result
        
        # å°è¯•ä»ç»“æœä¸­æå–tokenä½¿ç”¨é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        tokens_used = 0
        if isinstance(result, dict):
            tokens_used = result.get("tokens_used", 0)
        
        stats.end_step(step_name, tokens_used=tokens_used, status="completed")
        log.info(f"âœ… {step_name} å®Œæˆ")
        return True, step_instance, result
        
    except Exception as e:
        log.info(f"âŒ {step_name} å¼‚å¸¸: {e}")
        stats.end_step(step_name, status="failed")
        return False, None, {"error": str(e)}


def delete_step_files(config: PipelineConfig, step_name: str, target_episodes: Optional[List[str]] = None):
    """åˆ é™¤æŒ‡å®šæ­¥éª¤åŠå…¶ä¹‹åæ­¥éª¤çš„è¾“å‡ºæ–‡ä»¶"""
    output_root = config.project_root
    
    # å®šä¹‰æ¯ä¸ªæ­¥éª¤çš„é¢„æœŸè¾“å‡ºæ–‡ä»¶
    expected_files = {
        "0.1": ["0_1_timed_dialogue.srt"],
        "0.2": ["0_2_clues.json"],
        "0.3": ["global_character_graph_llm.json"],
        "0.4": ["0_4_calibrated_dialogue.txt"],
        "0.5": ["0_5_dialogue_turns.json"],
        "0.6": ["0_6_script_flow.json", "0_6_script_draft.md"],
        "0.7": ["0_7_script.stmf", "0_7_script_analysis.json"],
        "0.8": ["0_8_complete_screenplay.fountain", "0_8_complete_screenplay.fdx"]
    }
    
    # è·å–æ‰€æœ‰æ­¥éª¤åˆ—è¡¨
    all_steps = list(expected_files.keys())
    
    # æ‰¾åˆ°èµ·å§‹æ­¥éª¤çš„ç´¢å¼•
    if step_name not in all_steps:
        return
    
    start_index = all_steps.index(step_name)
    deleted_count = 0
    
    # å¤„ç†ä»èµ·å§‹æ­¥éª¤å¼€å§‹çš„æ‰€æœ‰æ­¥éª¤
    for i in range(start_index, len(all_steps)):
        current_step = all_steps[i]
        required_files = expected_files[current_step]
        
        if target_episodes:
            episodes = target_episodes
        else:
            episodes = PipelineUtils.get_episode_list(output_root)
        
        for episode_id in episodes:
            for filename in required_files:
                if current_step == "0.3":  # å…¨å±€æ–‡ä»¶ï¼ˆåœ¨ global/ ä¸‹ï¼‰
                    file_path = os.path.join(output_root, "global", filename)
                elif current_step == "0.8":  # å…¨å±€åˆå¹¶æ–‡ä»¶ï¼ˆé›†åˆæ ¹ç›®å½•ä¸‹ï¼‰
                    file_path = os.path.join(output_root, filename)
                else:  # å‰§é›†æ–‡ä»¶
                    file_path = os.path.join(output_root, episode_id, filename)
                
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        deleted_count += 1
                        log.info(f"ğŸ—‘ï¸ åˆ é™¤: {file_path}")
                    except Exception as e:
                        log.info(f"âŒ åˆ é™¤å¤±è´¥: {file_path} - {e}")
    
    log.info(f"âœ… åˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")


def check_invalid_files(config: PipelineConfig, step_name: str) -> List[str]:
    """æ£€æŸ¥æŒ‡å®šæ­¥éª¤çš„æ— æ•ˆæ–‡ä»¶"""
    output_root = config.project_root
    episodes = PipelineUtils.get_episode_list(output_root)
    
    # å®šä¹‰æ¯ä¸ªæ­¥éª¤çš„é¢„æœŸè¾“å‡ºæ–‡ä»¶
    expected_files = {
        "0.1": ["0_1_timed_dialogue.srt"],
        "0.2": ["0_2_clues.json"],
        "0.3": ["global_character_graph_llm.json"],
        "0.4": ["0_4_calibrated_dialogue.txt"],
        "0.5": ["0_5_dialogue_turns.json"],
        "0.6": ["0_6_script_flow.json", "0_6_script_draft.md"],
        "0.7": ["0_7_script.stmf", "0_7_script_analysis.json"],
        "0.8": ["0_8_complete_screenplay.fountain", "0_8_complete_screenplay.fdx"]
    }
    
    if step_name not in expected_files:
        return []
    
    required_files = expected_files[step_name]
    invalid_episodes = []
    
    for episode_id in episodes:
        for filename in required_files:
            if step_name == "0.3":  # å…¨å±€æ–‡ä»¶ï¼ˆåœ¨ global/ ä¸‹ï¼‰
                file_path = os.path.join(output_root, "global", filename)
            elif step_name == "0.8":  # å…¨å±€åˆå¹¶æ–‡ä»¶ï¼ˆé›†åˆæ ¹ç›®å½•ä¸‹ï¼‰
                file_path = os.path.join(output_root, filename)
            else:  # å‰§é›†æ–‡ä»¶
                file_path = os.path.join(output_root, episode_id, filename)
            
            if not os.path.exists(file_path):
                invalid_episodes.append(episode_id)
                break
            else:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–æŸå
                try:
                    if filename.endswith('.json'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            # é’ˆå¯¹ä¸åŒæ­¥éª¤çš„ç»“æ„æ ¡éªŒ
                            if step_name == "0.5":
                                valid = False
                                if isinstance(data, dict):
                                    # æ¥å— dialogue_turnsï¼ˆä¸»ï¼‰ã€turns/dialoguesï¼ˆå…¼å®¹ï¼‰
                                    valid = bool(data.get('dialogue_turns') or data.get('turns') or data.get('dialogues'))
                                elif isinstance(data, list):
                                    valid = len(data) > 0
                                if not valid:
                                    invalid_episodes.append(episode_id)
                                    break
                            else:
                                # é€šç”¨ï¼ˆå®½æ¾ï¼‰æ ¡éªŒï¼šéç©ºå³å¯
                                if not data:
                                    invalid_episodes.append(episode_id)
                                    break
                    elif os.path.getsize(file_path) < 100:  # æ–‡ä»¶å¤ªå°
                        invalid_episodes.append(episode_id)
                        break
                except (json.JSONDecodeError, Exception):
                    invalid_episodes.append(episode_id)
                    break
    
    return invalid_episodes


# å®šä¹‰ Pydantic æ¨¡å‹
class RunPipelineRequest(BaseModel):
    """è¿è¡Œæµæ°´çº¿è¯·æ±‚æ¨¡å‹"""
    collection: str
    start_step: str = "0.1"
    bucket: Optional[str] = None
    skip_integrity_check: bool = False
    execution_mode: ExecutionMode = ExecutionMode.RESUME
    target_episodes: Optional[List[str]] = []


class CollectionResponse(BaseModel):
    """é›†åˆå“åº”æ¨¡å‹"""
    collections: List[str]


class StepStatusResponse(BaseModel):
    """æ­¥éª¤çŠ¶æ€å“åº”æ¨¡å‹"""
    step: str
    description: str
    status: str
    invalid_files: List[str]
    valid_count: int
    total_count: int


class PipelineStatusResponse(BaseModel):
    """æµæ°´çº¿çŠ¶æ€å“åº”æ¨¡å‹"""
    collection: str
    steps: List[StepStatusResponse]
    output_dir: str


class PipelineResult(BaseModel):
    """æµæ°´çº¿æ‰§è¡Œç»“æœæ¨¡å‹"""
    success: bool
    message: str
    report_file: Optional[str] = None
    stats: Optional[Dict[str, Any]] = None


# FastAPI åº”ç”¨
app = FastAPI(title="å‰§é›†å‰§æœ¬ç”Ÿæˆæµæ°´çº¿ API", 
              description="æ”¯æŒä» Step 0.1 åˆ° Step 0.8 çš„å®Œæ•´æµæ°´çº¿æ‰§è¡Œï¼Œæ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼",
              version="1.0.0")


@app.get("/")
def read_root():
    return {"message": "å‰§é›†å‰§æœ¬ç”Ÿæˆæµæ°´çº¿ API", "version": "1.0.0"}


@app.get("/collections", response_model=CollectionResponse)
def get_collections():
    """è·å–å¯ç”¨é›†åˆåˆ—è¡¨"""
    base_output_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "new_pipeline", "output"))
    candidates = []
    
    try:
        for name in sorted(os.listdir(base_output_dir)):
            outer_path = os.path.join(base_output_dir, name)
            if not os.path.isdir(outer_path):
                continue
            inner_same = os.path.isdir(os.path.join(outer_path, name))
            has_global = os.path.isdir(os.path.join(outer_path, "global"))
            has_episode = False
            
            try:
                for d in os.listdir(outer_path):
                    if d.startswith("episode_") and os.path.isdir(os.path.join(outer_path, d)):
                        has_episode = True
                        break
            except Exception:
                pass
            
            if inner_same or has_global or has_episode:
                candidates.append(name)
    except FileNotFoundError:
        candidates = []
    
    return CollectionResponse(collections=candidates)


@app.post("/run_pipeline", response_model=PipelineResult)
async def run_pipeline(request: RunPipelineRequest):
    """è¿è¡Œæµæ°´çº¿"""
    # éªŒè¯èµ·å§‹æ­¥éª¤
    valid_steps = ["0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8"]
    if request.start_step not in valid_steps:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„èµ·å§‹æ­¥éª¤: {request.start_step}ï¼Œå¯ç”¨æ­¥éª¤: {', '.join(valid_steps)}")
    
    # é…ç½®
    config = PipelineConfig()
    base_output_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "new_pipeline", "output"))
    outer_path = os.path.join(base_output_dir, request.collection)
    inner_same_path = os.path.join(outer_path, request.collection)
    
    # å…¼å®¹åŒå±‚ä¸å•å±‚ç›®å½•ç»“æ„
    if os.path.isdir(inner_same_path):
        output_root = os.path.abspath(inner_same_path)
    else:
        output_root = os.path.abspath(outer_path)
    
    if not os.path.exists(output_root):
        raise HTTPException(status_code=404, detail=f"é›†åˆç›®å½•ä¸å­˜åœ¨: {output_root}")
    
    # å…è®¸å¤–éƒ¨è¦†ç›– bucket
    if request.bucket:
        config.config.setdefault("gcp", {})
        config.config["gcp"]["bucket_name"] = request.bucket

    config.config.setdefault("project", {})
    config.config["project"]["root_dir"] = output_root
    config.config["project"]["output_dir"] = output_root

    # æ£€æŸ¥è¾“å…¥ç›®å½•ï¼Œroot_dir
    log.info(f"é¡¹ç›®æ ¹ç›®å½•: {config.project_root}, è¾“å‡ºç›®å½•: {config.output_dir}")

    # ç¡®å®šç›®æ ‡å‰§é›†
    target_episodes = []
    if request.execution_mode == ExecutionMode.FIX_INVALID:
        # æ£€æŸ¥æ— æ•ˆæ–‡ä»¶
        invalid_episodes = check_invalid_files(config, request.start_step)
        if not invalid_episodes:
            log.info(f"âœ… æ­¥éª¤ {request.start_step} æ²¡æœ‰æ— æ•ˆæ–‡ä»¶")
            return PipelineResult(success=True, message=f"æ­¥éª¤ {request.start_step} æ²¡æœ‰æ— æ•ˆæ–‡ä»¶éœ€è¦ä¿®å¤")
        target_episodes = invalid_episodes
    elif request.target_episodes:
        target_episodes = request.target_episodes

    log.info("="*60)
    log.info("ğŸ¬ å‰§é›†å‰§æœ¬ç”Ÿæˆæµæ°´çº¿")
    log.info("="*60)
    log.info(f"è¾“å‡ºç›®å½•: {output_root}")
    log.info(f"é›†åˆåç§°: {request.collection}")
    log.info(f"èµ·å§‹æ­¥éª¤: {request.start_step}")
    log.info(f"æ‰§è¡Œæ¨¡å¼: {request.execution_mode}")
    log.info(f"è·³è¿‡å®Œæ•´æ€§æ£€æŸ¥: {request.skip_integrity_check}")
    if target_episodes:
        log.info(f"ç›®æ ‡å‰§é›†: {len(target_episodes)} ä¸ª")
    log.info("="*60)

    # åˆå§‹åŒ–ç»Ÿè®¡
    stats = PipelineStats()
    
    # å®šä¹‰æ­¥éª¤åˆ—è¡¨
    steps = [
        ("0.1", Step0_1ASR, "ASR"),
        ("0.2", Step0_2ClueExtraction, "çº¿ç´¢æå–"),
        ("0.3", Step0_3GlobalAlignmentLLM, "å…¨å±€è§’è‰²å¯¹é½"),
        ("0.4", Step0_4SpeakerCalibration, "è¯´è¯äººæ ¡å‡†"),
        ("0.5", Step0_5IntegratedAnalysis, "å¯¹è¯è½®æ¬¡é‡æ„"),
        ("0.6", Step0_6PlotExtraction, "æƒ…èŠ‚æŠ½å–"),
        ("0.7", Step0_7ScriptWriting, "å‰§æœ¬æ’°å†™ï¼ˆSTMFï¼‰"),
        ("0.8", Step0_8FinalScript, "åˆå¹¶ä¸å¯¼å‡º")
    ]
    
    # æ‰¾åˆ°èµ·å§‹æ­¥éª¤
    start_index = 0
    for i, (step_num, _, _) in enumerate(steps):
        if step_num == request.start_step:
            start_index = i
            break
    else:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„èµ·å§‹æ­¥éª¤: {request.start_step}")
    
    # å¤„ç†å¼ºåˆ¶é‡è·‘æ¨¡å¼
    if request.execution_mode == ExecutionMode.FORCE_RERUN:
        log.info(f"ğŸ—‘ï¸ å¼ºåˆ¶é‡è·‘æ¨¡å¼ï¼šåˆ é™¤æ­¥éª¤ {request.start_step} çš„ç°æœ‰æ–‡ä»¶...")
        log.info("âš ï¸ æ³¨æ„ï¼šå¼ºåˆ¶é‡è·‘å°†åˆ é™¤ç°æœ‰æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½é‡è¦æ•°æ®")
        delete_step_files(config, request.start_step, target_episodes)
    
    log.info(f"ä»æ­¥éª¤ {request.start_step} å¼€å§‹æ‰§è¡Œ ({request.execution_mode} æ¨¡å¼)...")
    
    # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
    report_generator = PipelineReportGenerator(config)
    
    try:
        # æ‰§è¡Œæ­¥éª¤
        for i in range(start_index, len(steps)):
            step_num, step_class, step_desc = steps[i]
            step_name = f"step{step_num.replace('.', '_')}"
            
            # è®°å½•æ­¥éª¤å¼€å§‹
            report_generator.record_step_start(step_name)
            
            # å¦‚æœè·³è¿‡å®Œæ•´æ€§æ£€æŸ¥ï¼Œåˆ™ä¿®æ”¹æ£€æŸ¥å‡½æ•°
            if request.skip_integrity_check:
                global check_file_integrity
                original_check = check_file_integrity
                check_file_integrity = lambda config, step_name: True
            
            success, step_instance, step_result = run_step(step_num, step_class, config, stats)
            
            # æ¢å¤åŸå§‹æ£€æŸ¥å‡½æ•°
            if request.skip_integrity_check:
                check_file_integrity = original_check
            
            # è®°å½•æ­¥éª¤ç»“æŸï¼ˆè·å–å®¢æˆ·ç«¯tokenä½¿ç”¨ä¿¡æ¯ï¼‰
            client = getattr(step_instance, 'client', None) if step_instance else None
            report_generator.record_step_end(step_name, step_result, client)
            
            if not success:
                log.info(f"\nâŒ æµæ°´çº¿åœ¨æ­¥éª¤ {step_num} å¤±è´¥")
                stats.print_summary()
                return PipelineResult(success=False, message=f"æµæ°´çº¿åœ¨æ­¥éª¤ {step_num} å¤±è´¥: {step_result}")
        
        log.info("\nğŸ‰ å…¨æµç¨‹å®Œæˆ")
        stats.print_summary()
        
        # ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š
        try:
            report_file = report_generator.generate_report(config.output_dir)
            log.info(f"\nğŸ“Š è¿è¡ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return PipelineResult(success=True, 
                                message="æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ", 
                                report_file=report_file, 
                                stats={ 
                                    "total_duration": time.time() - stats.total_start_time,
                                    "total_tokens": sum(s.get("tokens_used", 0) for s in stats.step_stats.values()),
                                    "step_stats": stats.step_stats
                                })
        except Exception as e:
            log.info(f"\nâš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return PipelineResult(success=True, 
                                message="æµæ°´çº¿æ‰§è¡ŒæˆåŠŸä½†æŠ¥å‘Šç”Ÿæˆå¤±è´¥", 
                                stats={ 
                                    "total_duration": time.time() - stats.total_start_time,
                                    "total_tokens": sum(s.get("tokens_used", 0) for s in stats.step_stats.values()),
                                    "step_stats": stats.step_stats
                                })
    
    except Exception as e:
        log.info(f"\nâŒ è¿è¡Œå¼‚å¸¸: {e}")
        stats.print_summary()
        return PipelineResult(success=False, message=f"è¿è¡Œå¼‚å¸¸: {e}")


@app.get("/check_status/{collection}", response_model=PipelineStatusResponse)
def check_pipeline_status(collection: str):
    """æ£€æŸ¥æŒ‡å®šé›†åˆçš„æµæ°´çº¿çŠ¶æ€"""
    base_output_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "new_pipeline", "output"))
    collection_path = os.path.join(base_output_dir, collection)
    inner_same_path = os.path.join(collection_path, collection)
    
    # å…¼å®¹åŒå±‚ä¸å•å±‚ç›®å½•ç»“æ„
    if os.path.isdir(inner_same_path):
        output_root = os.path.abspath(inner_same_path)
    else:
        output_root = os.path.abspath(collection_path)
    
    if not os.path.exists(output_root):
        raise HTTPException(status_code=404, detail=f"é›†åˆç›®å½•ä¸å­˜åœ¨: {output_root}")
    
    # é…ç½®
    config = PipelineConfig()
    config.config.setdefault("project", {})
    config.config["project"]["root_dir"] = output_root
    config.config["project"]["output_dir"] = output_root
    
    # æ£€æŸ¥å„æ­¥éª¤çš„æ–‡ä»¶çŠ¶æ€
    steps = [
        ("0.1", "ASR"),
        ("0.2", "çº¿ç´¢æå–"),
        ("0.3", "å…¨å±€è§’è‰²å¯¹é½"),
        ("0.4", "è¯´è¯äººæ ¡å‡†"),
        ("0.5", "å¯¹è¯è½®æ¬¡é‡æ„"),
        ("0.6", "æƒ…èŠ‚æŠ½å–"),
        ("0.7", "å‰§æœ¬æ’°å†™ï¼ˆSTMFï¼‰"),
        ("0.8", "åˆå¹¶ä¸å¯¼å‡º")
    ]
    
    step_statuses = []
    for step_num, step_desc in steps:
        invalid_files = check_invalid_files(config, step_num)
        total_episodes = len(PipelineUtils.get_episode_list(config.project_root))
        valid_count = total_episodes - len(invalid_files)
        
        if step_num == "0.3":  # å…¨å±€æ­¥éª¤
            candidates = [
                os.path.join(config.project_root, "global", "global_character_graph_llm.json"),
                os.path.join(config.project_root, "global_character_graph_llm.json"),
            ]
            if any(os.path.exists(p) for p in candidates):
                status = "âœ… å®Œæˆ"
            else:
                status = "âŒ æœªå®Œæˆ"
        else:
            if len(invalid_files) == 0:
                status = "âœ… å®Œæˆ"
            elif len(invalid_files) == total_episodes:
                status = "âŒ æœªå¼€å§‹"
            else:
                status = f"âš ï¸ éƒ¨åˆ†å®Œæˆ ({valid_count}/{total_episodes})"
        
        step_statuses.append(StepStatusResponse(
            step=step_num,
            description=step_desc,
            status=status,
            invalid_files=invalid_files,
            valid_count=valid_count,
            total_count=total_episodes
        ))
    
    return PipelineStatusResponse(
        collection=collection,
        steps=step_statuses,
        output_dir=output_root
    )


@app.get("/step_files/{collection}/{step_name}")
def check_step_files(collection: str, step_name: str):
    """æ£€æŸ¥æŒ‡å®šæ­¥éª¤çš„æ–‡ä»¶å®Œæ•´æ€§"""
    base_output_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "new_pipeline", "output"))
    collection_path = os.path.join(base_output_dir, collection)
    inner_same_path = os.path.join(collection_path, collection)
    
    # å…¼å®¹åŒå±‚ä¸å•å±‚ç›®å½•ç»“æ„
    if os.path.isdir(inner_same_path):
        output_root = os.path.abspath(inner_same_path)
    else:
        output_root = os.path.abspath(collection_path)
    
    if not os.path.exists(output_root):
        raise HTTPException(status_code=404, detail=f"é›†åˆç›®å½•ä¸å­˜åœ¨: {output_root}")
    
    # é…ç½®
    config = PipelineConfig()
    config.config.setdefault("project", {})
    config.config["project"]["root_dir"] = output_root
    config.config["project"]["output_dir"] = output_root
    
    # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
    is_valid = check_file_integrity(config, step_name)
    
    if is_valid:
        return {"step": step_name, "collection": collection, "valid": True, "message": f"æ­¥éª¤ {step_name} æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡"}
    else:
        invalid_files = check_invalid_files(config, step_name)
        return {"step": step_name, "collection": collection, "valid": False, "message": f"æ­¥éª¤ {step_name} æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥", "invalid_files": invalid_files}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)