import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from . import PipelineStep
from typing import Dict, List, Any
import json
from google import genai
from google.genai import types as gat
from google.oauth2 import service_account
from new_pipeline.steps.commont_log import log


class Step0_3GlobalAlignmentLLM(PipelineStep):
    """ä½¿ç”¨LLMè¿›è¡Œå…¨å±€å®žä½“å¯¹é½ä¸Žå…³ç³»æž„å»º - ç›´æŽ¥APIè°ƒç”¨ç‰ˆ"""

    def __init__(self, config):
        super().__init__(config)
        self._step_name = "Step0.3-LLM"
        self._description = "å…¨å±€å®žä½“å¯¹é½ï¼ˆLLMï¼‰"
        
        # ç›´æŽ¥åˆå§‹åŒ–Google GenAIå®¢æˆ·ç«¯ï¼Œç»•è¿‡client.py
        gcp_config = config.gcp_config
        self.genai_client = genai.Client(
            vertexai=True,
            project=gcp_config['project_id'],
            location=gcp_config['location'],
            credentials=service_account.Credentials.from_service_account_file(
                gcp_config['credentials_path'],
                scopes=['https://www.googleapis.com/auth/cloud-platform']
            )
        )

    @property
    def step_number(self) -> int:
        return 0.31

    @property
    def step_name(self) -> str:
        return self._step_name

    @property
    def description(self) -> str:
        return self._description

    def run(self) -> Dict[str, Any]:
        # ä½¿ç”¨çœŸå®žæ•°æ®çš„ç‰ˆæœ¬
        log.info("ðŸ” ä½¿ç”¨çœŸå®žå‰§é›†æ•°æ®çš„step0.3ï¼šç›´æŽ¥è°ƒç”¨Google GenAI API")
        
        # 1. è¯»å–é¢„å¤„ç†æ•°æ®ï¼ˆä»Žé…ç½®çš„è¾“å‡ºæ ¹ç›®å½•èŽ·å–ï¼Œè€Œéžç¡¬ç¼–ç ï¼‰
        output_root = self.config.project_root or self.config.output_dir
        preprocessed_file = os.path.join(output_root, 'global', '0_3_preprocessed_data.json')
        
        if not os.path.exists(preprocessed_file):
            log.info("âŒ é¢„å¤„ç†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨è‡ªåŠ¨èšåˆç”Ÿæˆ...")
            base_dir = output_root
            import glob
            clues = sorted(glob.glob(os.path.join(base_dir, 'episode_*', '0_2_clues.json')))
            episodes = []
            for fp in clues:
                try:
                    with open(fp, 'r', encoding='utf-8') as f:
                        j = json.load(f)
                        ep = os.path.basename(os.path.dirname(fp))
                        episodes.append({
                            'episode_id': ep,
                            'data': j
                        })
                except Exception as e:
                    log.info(f"âš ï¸ è¯»å–çº¿ç´¢å¤±è´¥ {fp}: {e}")
                continue
            os.makedirs(os.path.dirname(preprocessed_file), exist_ok=True)
            with open(preprocessed_file, 'w', encoding='utf-8') as f:
                json.dump({'episodes': episodes}, f, ensure_ascii=False, indent=2)
            log.info(f"âœ… å·²ç”Ÿæˆé¢„å¤„ç†æ–‡ä»¶: {preprocessed_file}ï¼Œå…± {len(episodes)} é›†")
        
        with open(preprocessed_file, 'r', encoding='utf-8') as f:
            preprocessed_data = json.load(f)
        
        log.info(f"âœ… è¯»å–é¢„å¤„ç†æ•°æ®ï¼š{len(preprocessed_data.get('episodes', []))} ä¸ªå‰§é›†")
        
        # 2. æž„å»ºçœŸå®žçš„prompt
        real_prompt = self._build_real_prompt(preprocessed_data)
        
        # å¢žå¼ºç‰ˆ schemaï¼šåœ¨å…¼å®¹åŸºç¡€ä¸Šå¢žåŠ  is_high_statusã€supportingã€ç»“æž„åŒ– hints
        simple_schema = {
            'type': 'OBJECT',
            'properties': {
                'characters': {
                    'type': 'ARRAY',
                    'items': {
                        'type': 'OBJECT',
                        'properties': {
                            'name': {'type': 'STRING'},
                            'description': {'type': 'STRING'},
                            'first_appearance_episode': {'type': 'STRING'},
                            'first_appearance_evidence': {'type': 'STRING'},
                            'is_high_status': {'type': 'BOOLEAN'},
                            'aliases': {'type': 'ARRAY', 'items': {'type': 'STRING'}}
                        }
                    }
                },
                'relationships': {
                    'type': 'ARRAY',
                    'items': {
                        'type': 'OBJECT',
                        'properties': {
                            'participants': {'type': 'ARRAY', 'items': {'type': 'STRING'}},
                            'relationship_type': {'type': 'STRING'},
                            'evidence': {'type': 'STRING'}
                        }
                    }
                },
                'per_episode': {
                    'type': 'ARRAY',
                    'items': {
                        'type': 'OBJECT',
                        'properties': {
                            'episode_id': {'type': 'STRING'},
                            'supporting': {
                                'type': 'ARRAY',
                                'items': {
                                    'type': 'OBJECT',
                                    'properties': {
                                        'label': {'type': 'STRING'},
                                        'title_or_role': {'type': 'STRING'},
                                        'affiliation_owner': {'type': 'STRING'},
                                        'affiliation_type': {'type': 'STRING'},
                                        'episode_traits': {'type': 'STRING'}
                                    }
                                }
                            },
                            'speaker_alignment_hints': {
                                'type': 'ARRAY',
                                'items': {
                                    'type': 'OBJECT',
                                    'properties': {
                                        'spk_full_id': {'type': 'STRING'},
                                        'candidate': {'type': 'STRING'},
                                        'confidence': {'type': 'NUMBER'},
                                        'sticky': {'type': 'BOOLEAN'},
                                        'role_type': {'type': 'STRING'},
                                        'affiliation_owner': {'type': 'STRING'},
                                        'rationale': {'type': 'STRING'}
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        
        try:
            # æ ¹æ®Context7æ–‡æ¡£ä¿®æ­£çš„APIè°ƒç”¨æ–¹å¼
            raw_response = self.genai_client.models.generate_content(
                model="gemini-2.5-pro",
                contents=real_prompt,
                config=gat.GenerateContentConfig(
                    system_instruction=(
                        "ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è§’è‰²å…³ç³»åˆ†æžä¸“å®¶ã€‚ä¸Šæ¸¸çº¿ç´¢å¯èƒ½åŒ…å«é”™è¯¯/ä¸ä¸€è‡´/é—æ¼ã€‚" \
                        "ä½ çš„ä»»åŠ¡ï¼šæž„å»ºå…¨å‰§ç»Ÿä¸€ä¸”å¯ç”¨äºŽåŽç»­æ ¡å‡†çš„å…¨çƒäººç‰©å›¾è°±ï¼Œå¹¶ä¸ºæ¯é›†ç”Ÿæˆå¯æ‰§è¡Œçš„ speaker_alignment_hintsã€‚" \
                        "\nä¸¥æ ¼è§„åˆ™ï¼š\n" \
                        "1) è¯æ®ä¼˜å…ˆçº§ï¼šäººåæ¡/æ˜Žç¡®å­—å¹• > è·¨é›†è§†è§‰ä¸€è‡´æ€§ > ç¨³å®šå…³ç³»ç½‘ç»œ > è¯­ä¹‰/è¯è¯­é£Žæ ¼ã€‚é¿å…è¿‡åº¦åˆå¹¶ï¼Œä»…åœ¨å¤šæ¡ç‹¬ç«‹è¯æ®ä¸€è‡´æ—¶æ‰åˆå¹¶åˆ«å/èº«ä»½ã€‚\n" \
                        "2) è§’è‰²ç‹¬ç«‹æ€§ï¼šåŠŸèƒ½æ€§é…è§’ï¼ˆæŠ¤å«/åŠ©ç†/å¸æœºç­‰ï¼‰å¦‚è¯æ®è¡¨æ˜Žæ˜¯ç‹¬ç«‹ä¸ªä½“ï¼Œåº”ä¿æŒç‹¬ç«‹ã€‚ä¸å¯å› ç§°è°“ï¼ˆå°å§/æ€»ï¼‰è€Œæ”¹å†™ display_nameã€‚\n" \
                        "3) é«˜åœ°ä½çº¦æŸï¼šå…·æœ‰äººåæ¡+å¤´è¡”ä¸”è¯æ®å……è¶³è€…è§†ä¸ºé«˜åœ°ä½è§’è‰²ï¼ˆcharacters[].is_high_status=trueï¼‰ï¼Œä¸å¾—é™çº§ä¸ºä»Žå±žã€‚\n" \
                        "4) è¯­ç”¨åŽŸåˆ™ï¼šä¸¥æ ¼åŒºåˆ†ç§°å‘¼ï¼ˆaddressï¼‰ä¸ŽæåŠï¼ˆmentionï¼‰ã€‚address è¡¨ç¤ºå½“å‰å¯¹è¯å¯¹è±¡ï¼›mention ä¸å»ºç«‹ä»Žå±žæˆ–ç›´æŽ¥å…³ç³»ã€‚\n" \
                        "5) spk æ ‡å‡†ï¼šspeaker_alignment_hints[].spk_full_id å¿…é¡»ä¸º spk_X_epNï¼ˆå¦‚ spk_0_ep1ï¼‰ã€‚\n" \
                        "6) é¦–æ¬¡å‡ºçŽ°ï¼šç»™å‡º first_appearance_episode ä¸Žæœ€å…·ä»£è¡¨æ€§çš„ first_appearance_evidenceï¼ˆä¸€å¥è¯è¯æ®ï¼‰ã€‚\n" \
                        "\nè¾“å‡ºè¦æ±‚ï¼ˆä¸¥æ ¼JSONï¼Œæ— è§£é‡Šï¼‰ï¼š\n" \
                        "- characters[]: name, description, first_appearance_episode, first_appearance_evidence, is_high_status(boolean), aliases[]\n" \
                        "- relationships[]: participants[], relationship_type, evidence\n" \
                        "- per_episode[]: episode_id, supporting[]({label,title_or_role,affiliation_owner,affiliation_type,episode_traits}),\n" \
                        "  speaker_alignment_hints[]({spk_full_id,candidate,confidence(0-1),sticky,role_type,affiliation_owner,rationale})\n"
                    ),
                    max_output_tokens=65535,  # ä½¿ç”¨æœ€å¤§tokené™åˆ¶
                    temperature=0.1,
                    response_mime_type="application/json",
                    response_schema=simple_schema
                )
            )
            
            log.info(f"ðŸ“¥ æ”¶åˆ°åŽŸå§‹å“åº”ï¼Œç±»åž‹: {type(raw_response)}")
            # å¢žåŠ æ—¥å¿—æ‰“å°å…¨éƒ¨çš„å“åº”å†…å®¹ï¼Œç”¨äºŽè°ƒè¯•ã€‚è¿™ä¸ªåœ°æ–¹ä½¿ç”¨debugæ¨¡å¼æ‰“å°
            log.debug(f"åŽŸå§‹å“åº”å†…å®¹: {raw_response}")
            
            # ç›´æŽ¥è§£æžå“åº”
            if hasattr(raw_response, 'text') and raw_response.text:
                try:
                    result = json.loads(raw_response.text)
                    log.info(f"âœ… JSONè§£æžæˆåŠŸï¼š{len(result.get('characters', []))} ä¸ªè§’è‰²")
                except json.JSONDecodeError as e:
                    log.info(f"âš ï¸ JSONè§£æžå¤±è´¥: {e}")
                    log.info(f"åŽŸå§‹æ–‡æœ¬: {raw_response.text[:500]}...")
                    result = {
                        "characters": [],
                        "relationships": [],
                        "per_episode": []
                    }
            else:
                log.info("âŒ å“åº”æ²¡æœ‰textå†…å®¹")
                result = {
                    "characters": [],
                    "relationships": [],
                    "per_episode": []
                }
            
        except Exception as e:
            log.info(f"âŒ ç›´æŽ¥APIè°ƒç”¨å¤±è´¥: {e}")
            result = {
                "characters": [],
                "relationships": [],
                "per_episode": []
            }
        
        # ç»“æžœè§„èŒƒåŒ–ï¼Œç¡®ä¿å­—æ®µä¸ŽåŽç»­æ­¥éª¤å…¼å®¹
        result = self._normalize_result(result)

        # debug: æ‰“å°è§„èŒƒåŒ–åŽçš„ç»“æžœ
        log.debug(f"è§„èŒƒåŒ–åŽçš„ç»“æžœ: {result}")
        
        # ä¿å­˜ç»“æžœ
        global_dir = os.path.join(output_root, 'global')
        os.makedirs(global_dir, exist_ok=True)
        output_file = os.path.join(global_dir, 'global_character_graph_llm.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # é¢å¤–å¯¼å‡ºï¼šæ¯é›† speaker æç¤º CSVï¼ˆä¾¿äºŽäººå·¥å®¡é˜…/æ‰‹æ”¹ï¼‰ï¼Œä»¥åŠå…¨å±€è§’è‰² CSV
        try:
            import csv
            # 1) å…¨å±€è§’è‰² CSV
            global_char_csv = os.path.join(global_dir, 'global_characters.csv')
            with open(global_char_csv, 'w', encoding='utf-8', newline='') as cf:
                writer = csv.DictWriter(cf, fieldnames=[
                    'canonical_name', 'aliases', 'traits', 'is_high_status',
                    'first_appearance_episode', 'first_appearance_evidence'
                ])
                writer.writeheader()
                for ch in result.get('characters', []) or []:
                    writer.writerow({
                        'canonical_name': ch.get('canonical_name') or ch.get('name') or '',
                        'aliases': ','.join(ch.get('aliases') or []),
                        'traits': ch.get('traits') or ch.get('description') or '',
                        'is_high_status': 'true' if ch.get('is_high_status') else 'false',
                        'first_appearance_episode': ch.get('first_appearance_episode') or '',
                        'first_appearance_evidence': ch.get('first_appearance_evidence') or ''
                    })
            # 2) åˆ†é›† speaker æç¤º CSV
            for pe in result.get('per_episode', []) or []:
                ep_id = pe.get('episode_id') or ''
                if not ep_id:
                    continue
                ep_dir = os.path.join(output_root, ep_id)
                os.makedirs(ep_dir, exist_ok=True)
                ep_csv = os.path.join(ep_dir, '0_3_speaker_hints.csv')
                with open(ep_csv, 'w', encoding='utf-8', newline='') as ef:
                    writer = csv.DictWriter(ef, fieldnames=[
                        'spk_full_id', 'candidate', 'confidence', 'sticky',
                        'role_type', 'affiliation_owner', 'evidence'
                    ])
                    writer.writeheader()
                    for h in pe.get('speaker_alignment_hints') or []:
                        writer.writerow({
                            'spk_full_id': h.get('spk_full_id') or '',
                            'candidate': h.get('candidate') or '',
                            'confidence': h.get('confidence') or 0,
                            'sticky': 'true' if h.get('sticky') else 'false',
                            'role_type': h.get('role_type') or '',
                            'affiliation_owner': h.get('affiliation_owner') or '',
                            'evidence': h.get('rationale') or ''
                        })
        except Exception as e:
            log.info(f"âš ï¸ å¯¼å‡ºCSVæ—¶å‡ºçŽ°é—®é¢˜: {e}")
        
        log.info(f"âœ… {self.step_name} å®Œæˆ: {output_file}")

        return {
            "status": "completed",
            "output_file": output_file,
            "characters_count": len(result.get('characters', [])),
            "relationships_count": len(result.get('relationships', [])),
        }

    def _build_real_prompt(self, preprocessed_data: Dict[str, Any]) -> str:
        """æž„å»ºä½¿ç”¨çœŸå®žæ•°æ®çš„promptï¼ˆè¦†ç›–å…¨å‰§é›†ï¼Œç®€æ´SI+UPï¼‰"""
        lines: List[str] = []
        
        # æ¦‚è¦ä¸Žç›®æ ‡
        lines.append("è¯·åŸºäºŽä»¥ä¸‹åˆ†é›†çº¿ç´¢ï¼Œå®Œæˆå…¨å±€äººç‰©å¯¹é½ä¸Žå…³ç³»æž„å»ºï¼Œå¹¶è¾“å‡ºä¸¥æ ¼JSONï¼ˆä½¿ç”¨ç»™å®šschemaï¼‰ï¼š")
        lines.append("")
        lines.append("ç›®æ ‡ï¼š")
        lines.append("1) å¯¹é½spkä¸ŽçœŸå®žè§’è‰²ï¼Œè·¨é›†åˆå¹¶ï¼ŒåŒåå˜ä½“å½’ä¸€")
        lines.append("2) æ•´ç†è§’è‰²æè¿°ä¸Žé¦–æ¬¡å‡ºåœºä¿¡æ¯")
        lines.append("3) æå–å…³é”®å…³ç³»ï¼ˆçˆ¶å¥³/å§å¦¹/æŠ¤å«/çˆ±æ…•/æ•Œå¯¹ç­‰ï¼‰")
        lines.append("4) ä¸ºæ¯é›†æä¾›speaker_alignment_hints")
        lines.append("")
        
        # å…ˆéªŒä¿¡æ¯
        prior = preprocessed_data.get('prior_info', {})
        if prior.get('characters'):
            lines.append("å…ˆéªŒè§’è‰²ä¿¡æ¯ï¼ˆéƒ¨åˆ†ï¼‰ï¼š")
            for ch in prior['characters'][:20]:
                lines.append(f"- {ch.get('canonical_name','')}: {ch.get('traits_brief','')}")
            lines.append("")
        
        # æ•°æ®æ‘˜è¦
        summary = preprocessed_data.get('summary', {})
        lines.append(f"æ•°æ®æ‘˜è¦ï¼š{summary.get('total_episodes', 0)}é›†ï¼Œ{summary.get('total_characters', 0)}ä¸ªè§’è‰²ï¼Œ{summary.get('total_speakers', 0)}ä¸ªè¯´è¯äºº")
        lines.append("")
        
        # å…¨é›†éåŽ†ï¼ˆä¸é™åˆ¶é›†æ•°ï¼‰
        for item in preprocessed_data.get('episodes', []):
            ep = item.get('episode_id','')
            d = item.get('data', {})
            lines.append(f"[Episode {ep}]")
            
            # è§’è‰²ï¼ˆåŽ‹ç¼©å­—æ®µï¼‰
            chars_comp = []
            for c in d.get('explicit_characters', []) or []:
                chars_comp.append({
                    'name': c.get('name',''),
                    'title_or_role': c.get('title_or_role',''),
                    'evidence_modality': c.get('evidence_modality','')
                })
            if chars_comp:
                lines.append(f"Characters: {json.dumps(chars_comp, ensure_ascii=False)}")
            
            # è¯´è¯äººï¼ˆåŽ‹ç¼©å­—æ®µï¼‰
            spk_comp = []
            for s in d.get('speaker_profiles', []) or []:
                spk_comp.append({
                    'id': s.get('id',''),
                    'visual_description': (s.get('visual_description','') or '')[:160],
                    'key_lines': (s.get('key_lines') or [])[:2] if isinstance(s.get('key_lines'), list) else []
                })
            if spk_comp:
                lines.append(f"Speakers: {json.dumps(spk_comp, ensure_ascii=False)}")
            
            # äº¤äº’ï¼ˆé€‚åº¦åŽ‹ç¼©ï¼Œæ‰©å¤§æ•°é‡ï¼‰
            inter_comp = []
            for inter in (d.get('observed_interactions', []) or [])[:20]:
                inter_comp.append({
                    'participants': inter.get('participants', []),
                    'summary': (inter.get('summary','') or '')[:160],
                    'action_type': inter.get('action_type',''),
                    'addressee': inter.get('addressee','')
                })
            if inter_comp:
                lines.append(f"Interactions: {json.dumps(inter_comp, ensure_ascii=False)}")
            
            lines.append("")
        
        # æ˜Žç¡®è¾“å‡ºè¦æ±‚
        lines.append("è¯·ç¡®ä¿ï¼š1) è§’è‰²ç‹¬ç«‹æ€§ï¼Œé¿å…è¿‡åº¦åˆå¹¶ï¼›2) ä¸¥æ ¼JSONï¼Œæ— è§£é‡Šæ–‡æœ¬ï¼›3) speaker_alignment_hints ä½¿ç”¨æ ‡å‡† spk_X_epNã€‚")
        
        return "\n".join(lines)

    def _normalize_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """è§„èŒƒåŒ–LLMè¿”å›žï¼Œä¿®æ­£æ‹¼å†™å¹¶è¡¥å……Step0.4æ‰€éœ€çš„å…³é”®å­—æ®µã€‚"""
        normalized: Dict[str, Any] = {
            "characters": [],
            "relationships": result.get("relationships", []) or [],
            "per_episode": []
        }
        # 1) è§’è‰²å­—æ®µè§„èŒƒåŒ–
        for ch in result.get("characters", []) or []:
            # ä¿®æ­£æ‹¼å†™: first_appearence_evidence -> first_appearance_evidence
            if "first_appearence_evidence" in ch and "first_appearance_evidence" not in ch:
                ch["first_appearance_evidence"] = ch.get("first_appearence_evidence")
            name = ch.get("name") or ch.get("canonical_name") or ""
            description = ch.get("description") or ch.get("traits") or ""
            normalized["characters"].append({
                "name": name,
                "description": description,
                "first_appearance_episode": ch.get("first_appearance_episode") or "",
                "first_appearance_evidence": ch.get("first_appearance_evidence") or "",
                # å…¼å®¹Step0.4æ‘˜è¦ä½¿ç”¨
                "canonical_name": name,
                "traits": description,
                "aliases": ch.get("aliases") or [],
                "is_high_status": bool(ch.get("is_high_status") or False)
            })
        
        # 2) per_episode ç»“æž„è§„èŒƒåŒ–
        for pe in result.get("per_episode", []) or []:
            ep_id = pe.get("episode_id") or ""
            hints = pe.get("speaker_alignment_hints") or []
            supporting = pe.get("supporting") or []
            # å°†å­—ç¬¦ä¸²æç¤ºåŒ…è£…ä¸ºç»“æž„åŒ–å¯¹è±¡ï¼Œä¾¿äºŽStep0.4æ¶ˆè´¹
            norm_hints = []
            for h in hints:
                if isinstance(h, dict):
                    # ä¿æŒæ—¢æœ‰ç»“æž„ï¼›å¹¶ç¡®ä¿å…³é”®é”®å­˜åœ¨
                    norm_hints.append({
                        "spk_full_id": h.get("spk_full_id") or h.get("spk") or "",
                        "candidate": h.get("candidate") or "",
                        "confidence": h.get("confidence") or 0,
                        "sticky": bool(h.get("sticky") or False),
                        "role_type": h.get("role_type") or "",
                        "affiliation_owner": h.get("affiliation_owner") or "",
                        "rationale": h.get("rationale") or ""
                    })
                else:
                    # å­—ç¬¦ä¸²åœºæ™¯ï¼Œå°è¯•ç®€å•è§£æž spk/candidateï¼›å¦åˆ™ä»…ç™»è®° spk_full_id
                    text = str(h)
                    cand = ""
                    spk = text
                    # ç®€å•æ ¼å¼: "spk_1_ep3 -> è§’è‰²å"
                    if "->" in text:
                        parts = [p.strip() for p in text.split("->", 1)]
                        if len(parts) == 2:
                            spk, cand = parts[0], parts[1]
                    norm_hints.append({
                        "spk_full_id": spk,
                        "candidate": cand,
                        "confidence": 0,
                        "sticky": False,
                        "role_type": "",
                        "affiliation_owner": "",
                        "rationale": ""
                    })
            normalized["per_episode"].append({
                "episode_id": ep_id,
                "speaker_alignment_hints": norm_hints,
                "supporting": [
                    {
                        "label": (s or {}).get("label", ""),
                        "title_or_role": (s or {}).get("title_or_role", ""),
                        "affiliation_owner": (s or {}).get("affiliation_owner", ""),
                        "affiliation_type": (s or {}).get("affiliation_type", ""),
                        "episode_traits": (s or {}).get("episode_traits", "")
                    } for s in supporting if isinstance(s, dict)
                ]
            })
        return normalized
