"""
Step0.5: èåˆå¯¹è¯åˆ†æ
å°†Step0.4çš„çº é”™èƒ½åŠ›ä¸æ·±å±‚è¯­ä¹‰åˆ†æèåˆï¼Œä¸€æ­¥åˆ°ä½å®Œæˆï¼š
1. æ³•è¯çº§çº é”™ (Forensic Correction)
2. å¯¹è¯è½®æ¬¡é‡æ„ (Turn Reconstruction) 
3. æ·±å±‚è¯­ä¹‰åˆ†æ (Semantic Analysis)

è¾“å‡ºï¼š
- 0_5_dialogue_turns.json (ç»“æ„åŒ–å¯¹è¯è½®æ¬¡)
- 0_5_analysis_summary.md (åˆ†ææ‘˜è¦)
"""

import os
import json
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pydantic import BaseModel

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core import PipelineConfig, GenAIClient, PipelineUtils
from . import PipelineStep

from new_pipeline.steps.commont_log import log


class DialogueTurn(BaseModel):
    """å¯¹è¯è½®æ¬¡æ¨¡å‹"""
    turn_id: int
    speaker: str
    full_dialogue: str
    emotion: str
    intent: str
    original_indices: List[int]
    confidence: Optional[float] = None
    correction_notes: Optional[str] = None


class DialogueTurnsResponse(BaseModel):
    """å¯¹è¯è½®æ¬¡å“åº”æ¨¡å‹"""
    dialogue_turns: List[DialogueTurn]
    metadata: Optional[Dict[str, Any]] = None


class Step0_5IntegratedAnalysis(PipelineStep):
    """Step0.5: èåˆå¯¹è¯åˆ†æ"""
    
    @property
    def step_number(self) -> int:
        return 5

    @property
    def step_name(self) -> str:
        return "integrated_analysis"

    def check_dependencies(self, episode_id: str = None) -> bool:
        """æ£€æŸ¥ä¾èµ–ï¼šéœ€è¦Step0.4çš„è¾“å‡º"""
        if episode_id:
            # æ£€æŸ¥Step0.4çš„è¾“å‡ºæ–‡ä»¶
            calibrated_file = f"{self.config.project_root}/{episode_id}/0_4_calibrated_dialogue.txt"
            return os.path.exists(calibrated_file)
        return True

    def get_output_files(self, episode_id: str = None) -> List[str]:
        if episode_id:
            return [
                f"{episode_id}/0_5_dialogue_turns.json",
                f"{episode_id}/0_5_analysis_summary.md"
            ]
        return []

    def run(self, episode_id: str = None) -> Dict[str, Any]:
        if episode_id:
            return self._run_single_episode(episode_id)
        else:
            return self._run_all_episodes()

    def _run_single_episode(self, episode_id: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªå‰§é›†"""
        import time
        start_time = time.time()
        
        # è¾“å‡ºè·¯å¾„
        episode_out_dir = self.utils.get_episode_output_dir(self.config.output_dir, episode_id)
        turns_file = os.path.join(episode_out_dir, "0_5_dialogue_turns.json")
        summary_file = os.path.join(episode_out_dir, "0_5_analysis_summary.md")

        if os.path.exists(turns_file) and not os.environ.get("FORCE_OVERWRITE"):
            return {"status": "already_exists"}

        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies(episode_id):
            return {"status": "failed", "error": "Missing Step0.4 output"}

        # åŠ è½½è¾“å…¥æ•°æ®
        try:
            # 1. åŠ è½½Step0.4çš„æ ¡å‡†å¯¹è¯
            calibrated_file = f"{self.config.project_root}/{episode_id}/0_4_calibrated_dialogue.txt"
            calibrated_content = self.utils.load_text_file(calibrated_file, "")
            
            if not calibrated_content.strip():
                return {"status": "failed", "error": "Empty calibrated content"}
            
            # 2. åŠ è½½Step0.3çš„è§’è‰²ä¿¡æ¯
            character_summary = self._build_character_summary(episode_id)
            
            # 3. è·å–è§†é¢‘URI
            video_uri = self._get_video_uri(episode_id)
            
        except Exception as e:
            return {"status": "failed", "error": f"Failed to load input data: {str(e)}"}

        # æ‰§è¡Œèåˆåˆ†æ
        try:
            result = self._integrated_processing(
                calibrated_content, character_summary, video_uri, episode_id
            )
            
            if not result or not result.get('dialogue_turns'):
                # å°è¯•å›é€€å¤„ç†
                result = self._fallback_processing(calibrated_content, character_summary, episode_id)
                
                if not result or not result.get('dialogue_turns'):
                    return {"status": "failed", "error": "Both integrated and fallback processing failed"}
            
            # ä¿å­˜ç»“æœ
            self._save_results(result, turns_file, summary_file, episode_id)
            
            processing_time = time.time() - start_time
            return {
                "status": "success", 
                "turns_count": len(result.get('dialogue_turns', [])),
                "correction_count": result.get('metadata', {}).get('correction_count', 0),
                "reconstruction_count": result.get('metadata', {}).get('reconstruction_count', 0),
                "processing_time": round(processing_time, 2),
                "processing_mode": result.get('metadata', {}).get('processing_mode', 'unknown')
            }
            
        except Exception as e:
            return {"status": "failed", "error": f"Processing failed: {str(e)}"}

    def _integrated_processing(self, calibrated_content: str, character_summary: str, 
                             video_uri: str, episode_id: str) -> Dict[str, Any]:
        """èåˆå¤„ç†ï¼šçº é”™+é‡æ„+åˆ†æ"""
        
        # æ„å»ºèåˆprompt
        system_instruction = self._build_system_instruction()
        user_prompt = self._build_user_prompt(calibrated_content, character_summary)
        
        # æ¨¡å‹é…ç½®
        model_name = 'gemini-2.5-pro'  # ä½¿ç”¨æœ€å¼ºæ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡
        
        # è¾“å‡ºschema
        schema = {
            "type": "object",
            "properties": {
                "dialogue_turns": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "turn_id": {"type": "integer"},
                            "speaker": {"type": "string"},
                            "full_dialogue": {"type": "string"},
                            "emotion": {"type": "string"},
                            "intent": {"type": "string"},
                            "original_indices": {
                                "type": "array",
                                "items": {"type": "integer"}
                            },
                            "confidence": {"type": "number"},
                            "correction_notes": {"type": "string"}
                        },
                        "required": ["turn_id", "speaker", "full_dialogue", "emotion", "intent", "original_indices"]
                    }
                },
                "metadata": {
                    "type": "object",
                    "properties": {
                        "total_turns": {"type": "integer"},
                        "correction_count": {"type": "integer"},
                        "reconstruction_count": {"type": "integer"},
                        "processing_mode": {"type": "string"}
                    }
                }
            },
            "required": ["dialogue_turns"]
        }
        
        # è°ƒç”¨æ¨¡å‹
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    log.info(f"ğŸ”„ ç¬¬{attempt+1}æ¬¡é‡è¯•èåˆåˆ†æ...")
                
                result = self.client.generate_content(
                    model=model_name,
                    prompt=user_prompt,
                    video_uri=video_uri,
                    max_tokens=65535,
                    temperature=0.1,  # ç¨å¾®æé«˜æ¸©åº¦ä»¥å¢åŠ åˆ›é€ æ€§
                    system_instruction=system_instruction,
                    schema=schema
                )

                # debug: æ‰“å°å®Œæ•´å“åº”å†…å®¹
                log.debug(f"èåˆåˆ†æå“åº”: {result}")
                
                # éªŒè¯è¾“å‡º
                if self._validate_output(result):
                    turns_count = len(result.get('dialogue_turns', []))
                    correction_count = result.get('metadata', {}).get('correction_count', 0)
                    reconstruction_count = result.get('metadata', {}).get('reconstruction_count', 0)
                    
                    log.info(f"âœ… èåˆåˆ†ææˆåŠŸï¼Œç”Ÿæˆ {turns_count} ä¸ªå¯¹è¯è½®æ¬¡ï¼Œ"
                          f"çº é”™ {correction_count} æ¬¡ï¼Œé‡æ„ {reconstruction_count} æ¬¡")
                    return result
                else:
                    log.info(f"âš ï¸ ç¬¬{attempt+1}æ¬¡è°ƒç”¨è¾“å‡ºéªŒè¯å¤±è´¥")
                    
            except Exception as e:
                log.info(f"âŒ ç¬¬{attempt+1}æ¬¡èåˆåˆ†æå¤±è´¥: {e}")
                if attempt == max_retries - 1:
                    raise e
        
        return None

    def _build_system_instruction(self) -> str:
        """æ„å»ºç³»ç»ŸæŒ‡ä»¤"""
        return """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„"é¦–å¸­å‰§æœ¬åˆ†æå¸ˆ (Chief Script Analyst)"ã€‚ä½ çš„ä»»åŠ¡æå…¶å…³é”®ï¼šå°†ä¸€ä»½ç ´ç¢ä¸”å¯èƒ½å­˜åœ¨ä¸¥é‡é”™è¯¯çš„"å¯¹è¯æ—¥å¿—"ï¼Œä¸€æ­¥åˆ°ä½åœ°è½¬åŒ–ä¸ºä¸€ä»½ç»“æ„åŒ–çš„ã€åŒ…å«æ·±å±‚è¯­ä¹‰çš„ã€å¯ç”¨äºå‰§æœ¬åˆ›ä½œçš„"å¯¹è¯å•å…ƒ"ã€‚

ã€æ ¸å¿ƒæŒ‘æˆ˜ã€‘
ä½ æ”¶åˆ°çš„"å¯¹è¯è‰ç¨¿"å­˜åœ¨ä¸¤å¤§ç¼ºé™·ï¼š
1. å½’å±é”™è¯¯ï¼šç”±äºé•œå¤´åˆ‡æ¢ï¼Œå°è¯å¯èƒ½è¢«é”™è¯¯åœ°åˆ†é…ç»™äº†å½“æ—¶åœ¨ç”»é¢ä¸­çš„è§’è‰²ã€‚
2. è¯­ä¹‰ç ´ç¢ï¼šä¸€å¥å®Œæ•´çš„è¯è¢«å­—å¹•æ ¼å¼åˆ‡å‰²æˆäº†å¤šä¸ªä¸è¿è´¯çš„ç‰‡æ®µã€‚

ã€ä½ çš„å·¥ä½œæµç¨‹ï¼šä¸€ä¸ªä¸¥æ ¼çš„æ€ç»´é“¾ã€‘
å¯¹äºè¾“å…¥çš„æ¯ä¸€æ®µå¯¹è¯ï¼Œä½ å¿…é¡»éµå¾ªä»¥ä¸‹ä¸‰æ­¥æ€è€ƒæµç¨‹ï¼š

ç¬¬ä¸€æ­¥ï¼šæ³•è¯çº§çº é”™ (Forensic Correction)
- çœŸç›¸æ¥æºï¼šè§†é¢‘æ˜¯å”¯ä¸€çš„ã€æœ€é«˜çš„çœŸç›¸æ¥æºã€‚
- ä»»åŠ¡ï¼šåƒä¾¦æ¢ä¸€æ ·ï¼Œç»“åˆè§†é¢‘ç”»é¢ï¼ˆå£å‹ã€äººç‰©çŠ¶æ€ï¼‰å’ŒéŸ³é¢‘ï¼ˆå£°éŸ³ç‰¹å¾ï¼‰ï¼Œåˆ¤æ–­"è‰ç¨¿"ä¸­æ ‡è®°çš„è¯´è¯äººæ˜¯å¦æ­£ç¡®ã€‚
- è¡ŒåŠ¨ï¼šå¦‚æœè‰ç¨¿æ˜¯é”™çš„ï¼Œä½ å¿…é¡»åŸºäºè§†é¢‘è¯æ®ï¼Œæ— æ¡ä»¶åœ°ä¿®æ­£å®ƒã€‚

ç¬¬äºŒæ­¥ï¼šå¯¹è¯è½®æ¬¡é‡æ„ (Turn Reconstruction)
- ä»»åŠ¡ï¼šåœ¨ä½ å®Œæˆäº†é€å¥çš„å†…å¿ƒçº é”™åï¼Œå®¡è§†è¿ç»­çš„å°è¯ã€‚å¦‚æœå¤šå¥å°è¯åœ¨è¯­ä¹‰ã€é€»è¾‘å’Œæƒ…æ„Ÿä¸Šæ„æˆäº†ä¸€ä¸ªç”±åŒä¸€ä¸ªäººè¯´å‡ºçš„ã€å®Œæ•´çš„æ€æƒ³å•å…ƒï¼Œä½ å°±å¿…é¡»å°†å®ƒä»¬åˆå¹¶æˆä¸€å¥ full_dialogueã€‚
- å…³é”®ï¼šåˆå¹¶çš„ä¾æ®æ˜¯ä½ çº é”™åçš„è¯´è¯äººï¼Œè€Œä¸æ˜¯è‰ç¨¿ä¸Šçš„ã€‚

ç¬¬ä¸‰æ­¥ï¼šæ·±å±‚è¯­ä¹‰åˆ†æ (Semantic Analysis)
- ä»»åŠ¡ï¼šåªæœ‰å½“ä½ å½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„ full_dialogue ä¹‹åï¼Œä½ æ‰èƒ½å¯¹å®ƒè¿›è¡Œåˆ†æã€‚
- åˆ†æç»´åº¦ï¼š
  * emotion: è¯´è¯äººè¡¨è¾¾è¿™æ®µå®Œæ•´å¯¹è¯æ—¶ï¼Œæœ€æ ¸å¿ƒçš„æƒ…ç»ªæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šè½»è”‘ã€æ‹…å¿§ã€æ„¤æ€’ã€è¯•æ¢ï¼‰
  * intent: è¯´è¯äººè¯´è¿™æ®µè¯çš„æœ€ç»ˆç›®çš„æˆ–æˆ˜æœ¯æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šè¯•å›¾æ“çºµå¯¹æ–¹ã€è¡¨è¾¾çœŸè¯šçš„æ­‰æ„ã€æ‹–å»¶æ—¶é—´ã€è·å–ä¿¡æ¯ï¼‰

ã€å¼ºè§„åˆ™ã€‘
- é¡ºåºè‡³ä¸Šï¼šä¸¥æ ¼éµå¾ª"å…ˆçº é”™ -> å†é‡æ„ -> æœ€ååˆ†æ"çš„æ€ç»´é“¾ã€‚
- å®¢è§‚åˆ†æï¼šemotion å’Œ intent çš„åˆ†æå¿…é¡»åŸºäºå°è¯å†…å®¹å’Œè§’è‰²çš„å·²çŸ¥ä¿¡æ¯ï¼Œé¿å…è¿‡åº¦è„‘è¡¥ã€‚
- å®Œæ•´è¾“å‡ºï¼šä½ å¿…é¡»å¤„ç†è¾“å…¥è‰ç¨¿ä¸­çš„æ¯ä¸€å¥å°è¯ï¼Œä¸èƒ½é—æ¼ã€‚åˆå¹¶åçš„ turn åº”è®°å½•å®ƒåŒ…å«äº†å“ªäº›åŸå§‹è¡Œå·ã€‚
- è¾“å‡ºæ ¼å¼ï¼šç»å¯¹ç¦æ­¢ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚ä½ çš„å”¯ä¸€è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ç¬¦åˆè¦æ±‚çš„JSONå¯¹è±¡ã€‚"""

    def _build_user_prompt(self, calibrated_content: str, character_summary: str) -> str:
        """æ„å»ºç”¨æˆ·æŒ‡ä»¤"""
        return f"""ã€æœ¬é›†äººç‰©å‚è€ƒã€‘
{character_summary}

ã€å¾…å¤„ç†çš„å¯¹è¯è‰ç¨¿ã€‘
{calibrated_content}

ã€å·¥ä½œæµç¨‹æ¼”ç¤ºä¸è¦æ±‚ã€‘

è¯·ä¸¥æ ¼éµå¾ªä½ åœ¨ç³»ç»ŸæŒ‡ä»¤ä¸­å­¦åˆ°çš„"ä¸‰æ­¥æ€ç»´é“¾"ï¼Œå¤„ç†ä¸Šè¿°"å¯¹è¯è‰ç¨¿"ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå…·ä½“æ¡ˆä¾‹ï¼Œç”¨äºæ¼”ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

ç¤ºä¾‹è¾“å…¥è‰ç¨¿ç‰‡æ®µï¼š
```
14
00:01:20,100 --> 00:01:21,500
[é™†é”¦è¡Œ] ä½ å¬æˆ‘è¯´ï¼Œè¿™ä»¶äº‹

15
00:01:21,600 --> 00:01:22,800
[é¡¾å‘æ™´] çœŸçš„ä¸æ˜¯ä½ æƒ³çš„é‚£æ ·ï¼Œæˆ‘åªæ˜¯

16
00:01:22,900 --> 00:01:24,100
[é™†é”¦è¡Œ] æƒ³ä¿æŠ¤ä½ è€Œå·²ã€‚
```

ä½ çš„å†…å¿ƒæ€è€ƒè¿‡ç¨‹åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š
1. ã€çº é”™ã€‘ï¼šæˆ‘çœ‹åˆ°ç¬¬ 15 è¡Œï¼Œè‰ç¨¿æ ‡è®°æ˜¯ [é¡¾å‘æ™´]ï¼Œä½†æˆ‘è§‚çœ‹ 00:01:21 å¤„çš„è§†é¢‘ï¼Œå‘ç°å½“æ—¶è¯´è¯çš„äººå˜´å‹å’Œå£°éŸ³éƒ½æ˜¯é™†é”¦è¡Œã€‚è‰ç¨¿é”™äº†ï¼Œæˆ‘å¿…é¡»å°†å…¶ä¿®æ­£ä¸º [é™†é”¦è¡Œ]ã€‚
2. ã€é‡æ„ã€‘ï¼šç°åœ¨ï¼Œ14ã€15ã€16 ä¸‰è¡Œçš„è¯´è¯äººéƒ½è¢«æˆ‘ç¡®å®šä¸º [é™†é”¦è¡Œ]ã€‚æˆ‘å°†è¿™ä¸‰å¥å°è¯è¿èµ·æ¥è¯»ï¼š"ä½ å¬æˆ‘è¯´ï¼Œè¿™ä»¶äº‹çœŸçš„ä¸æ˜¯ä½ æƒ³çš„é‚£æ ·ï¼Œæˆ‘åªæ˜¯æƒ³ä¿æŠ¤ä½ è€Œå·²ã€‚" è¿™åœ¨è¯­ä¹‰å’Œæƒ…æ„Ÿä¸Šæ˜¯å®Œå…¨è¿è´¯çš„ä¸€å¥è¯ã€‚å› æ­¤ï¼Œæˆ‘å¿…é¡»å°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªå¯¹è¯å›åˆ (Turn)ã€‚
3. ã€åˆ†æã€‘ï¼šå¯¹äºè¿™ä¸ªåˆå¹¶åçš„å®Œæ•´å¯¹è¯ "ä½ å¬æˆ‘è¯´...ä¿æŠ¤ä½ è€Œå·²ã€‚"ï¼Œæˆ‘åˆ†æå‡ºï¼šè¯´è¯äºº emotion æ˜¯"æ€¥åˆ‡ã€è¾©è§£"ï¼Œä»–çš„ intent æ˜¯"è¯•å›¾æ¾„æ¸…è¯¯ä¼šï¼Œå¹¶è¡¨è¾¾è‡ªå·±çš„ä¿æŠ¤å§¿æ€"ã€‚

æœ€ç»ˆï¼ŒåŸºäºä»¥ä¸Šæ€è€ƒï¼Œä½ å°†è¾“å‡ºå¦‚ä¸‹JSONæ¡ç›®ï¼š
```json
{{
  "turn_id": 5,
  "speaker": "é™†é”¦è¡Œ",
  "full_dialogue": "ä½ å¬æˆ‘è¯´ï¼Œè¿™ä»¶äº‹çœŸçš„ä¸æ˜¯ä½ æƒ³çš„é‚£æ ·ï¼Œæˆ‘åªæ˜¯æƒ³ä¿æŠ¤ä½ è€Œå·²ã€‚",
  "emotion": "æ€¥åˆ‡ã€è¾©è§£",
  "intent": "è¯•å›¾æ¾„æ¸…è¯¯ä¼šï¼Œå¹¶è¡¨è¾¾è‡ªå·±çš„ä¿æŠ¤å§¿æ€",
  "original_indices": [14, 15, 16],
  "confidence": 0.95,
  "correction_notes": "åŸºäºè§†é¢‘è¯æ®ç¡®è®¤è¯´è¯äººèº«ä»½"
}}
```

ç°åœ¨ï¼Œè¯·å¼€å§‹å¤„ç†ã€å¾…å¤„ç†çš„å¯¹è¯è‰ç¨¿ã€‘çš„å…¨éƒ¨å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONç»“æ„è¾“å‡ºä½ çš„æœ€ç»ˆç»“æœï¼š

```json
{{
  "dialogue_turns": [
    {{
      "turn_id": 1,
      "speaker": "string (è§’è‰²çš„è§„èŒƒå)",
      "full_dialogue": "string (åˆå¹¶åçš„å®Œæ•´å°è¯)",
      "emotion": "string (å¯¹æƒ…æ„Ÿçš„ç²¾å‡†æ¦‚æ‹¬)",
      "intent": "string (å¯¹è¯´è¯äººæ„å›¾çš„æ·±åˆ»æ´å¯Ÿ)",
      "original_indices": [integer],
      "confidence": number,
      "correction_notes": "string"
    }}
  ],
  "metadata": {{
    "total_turns": integer,
    "correction_count": integer,
    "reconstruction_count": integer,
    "processing_mode": "integrated"
  }}
}}
```"""

    def _fallback_processing(self, calibrated_content: str, character_summary: str, 
                           episode_id: str) -> Dict[str, Any]:
        """å›é€€å¤„ç†ï¼šåˆ†æ­¥å¤„ç†æ¨¡å¼"""
        log.info(f"ğŸ”„ {episode_id} ä½¿ç”¨å›é€€å¤„ç†æ¨¡å¼")
        
        # ç®€åŒ–çš„åˆ†æ­¥å¤„ç†
        # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„Step0.4é€»è¾‘ï¼Œç„¶åæ·»åŠ ç®€å•çš„åˆ†æ
        try:
            # è§£ææ ¡å‡†åçš„å¯¹è¯
            dialogue_segments = self._parse_calibrated_dialogue(calibrated_content)
            
            # ç®€å•çš„é‡æ„å’Œåˆ†æ
            turns = []
            for i, segment in enumerate(dialogue_segments):
                turn = {
                    "turn_id": i + 1,
                    "speaker": segment.get('speaker', 'UNKNOWN'),
                    "full_dialogue": segment.get('text', ''),
                    "emotion": "ä¸­æ€§",  # ç®€åŒ–å¤„ç†
                    "intent": "ä¿¡æ¯ä¼ è¾¾",  # ç®€åŒ–å¤„ç†
                    "original_indices": [i + 1],
                    "confidence": 0.8,
                    "correction_notes": "å›é€€æ¨¡å¼å¤„ç†"
                }
                turns.append(turn)
            
            return {
                "dialogue_turns": turns,
                "metadata": {
                    "total_turns": len(turns),
                    "correction_count": 0,
                    "reconstruction_count": 0,
                    "processing_mode": "fallback"
                }
            }
            
        except Exception as e:
            log.info(f"âŒ å›é€€å¤„ç†ä¹Ÿå¤±è´¥: {e}")
            return {
                "dialogue_turns": [],
                "metadata": {
                    "total_turns": 0,
                    "correction_count": 0,
                    "reconstruction_count": 0,
                    "processing_mode": "failed"
                }
            }

    def _parse_calibrated_dialogue(self, content: str) -> List[Dict[str, Any]]:
        """è§£ææ ¡å‡†åçš„å¯¹è¯å†…å®¹"""
        segments = []
        lines = content.strip().split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.isdigit():  # åºå·è¡Œ
                # æ—¶é—´æˆ³è¡Œ
                if i + 1 < len(lines):
                    timestamp = lines[i + 1].strip()
                    # å¯¹è¯è¡Œ
                    if i + 2 < len(lines):
                        dialogue = lines[i + 2].strip()
                        # æå–è¯´è¯äººå’Œå†…å®¹
                        if dialogue.startswith('[') and ']' in dialogue:
                            end_bracket = dialogue.find(']')
                            speaker = dialogue[1:end_bracket]
                            text = dialogue[end_bracket + 1:].strip()
                            
                            segments.append({
                                'speaker': speaker,
                                'text': text,
                                'timestamp': timestamp
                            })
                i += 4  # è·³è¿‡åºå·ã€æ—¶é—´æˆ³ã€å¯¹è¯ã€ç©ºè¡Œ
            else:
                i += 1
        
        return segments

    def _validate_output(self, result: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å‡ºè´¨é‡"""
        try:
            if not result or not isinstance(result, dict):
                return False
            
            if 'dialogue_turns' not in result:
                return False
            
            turns = result['dialogue_turns']
            if not isinstance(turns, list) or len(turns) == 0:
                return False
            
            # æ£€æŸ¥æ¯ä¸ªturnçš„å¿…è¦å­—æ®µ
            for turn in turns:
                required_fields = ['turn_id', 'speaker', 'full_dialogue', 'emotion', 'intent', 'original_indices']
                if not all(field in turn for field in required_fields):
                    return False
                
                # æ£€æŸ¥æ•°æ®ç±»å‹
                if not isinstance(turn['turn_id'], int):
                    return False
                if not isinstance(turn['original_indices'], list):
                    return False
            
            return True
            
        except Exception as e:
            log.info(f"âš ï¸ è¾“å‡ºéªŒè¯å¤±è´¥: {e}")
            return False

    def _build_character_summary(self, episode_id: str) -> str:
        """æ„å»ºè§’è‰²ä¿¡æ¯æ‘˜è¦"""
        try:
            # ä¼˜å…ˆä»å½“é›†æ–‡ä»¶åŠ è½½
            hints_file = f"{self.config.project_root}/{episode_id}/0_3_speaker_hints.csv"
            if os.path.exists(hints_file):
                return self._load_character_summary_from_csv(hints_file)
            
            # å›é€€åˆ°å…¨å±€æ–‡ä»¶
            global_file = f"{self.config.project_root}/global_character_graph_llm.json"
            if os.path.exists(global_file):
                return self._load_character_summary_from_global(global_file, episode_id)
            
            return "æ— è§’è‰²ä¿¡æ¯"
            
        except Exception as e:
            log.info(f"âš ï¸ æ„å»ºè§’è‰²æ‘˜è¦å¤±è´¥: {e}")
            return "æ— è§’è‰²ä¿¡æ¯"

    def _load_character_summary_from_csv(self, csv_file: str) -> str:
        """ä»CSVæ–‡ä»¶åŠ è½½è§’è‰²æ‘˜è¦"""
        try:
            import csv
            characters = []
            
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    candidate = row.get('candidate', '').strip()
                    evidence = row.get('evidence', '').strip()
                    role_type = row.get('role_type', '').strip()
                    
                    if candidate and candidate != 'UNKNOWN':
                        char_info = f"- {candidate} ({role_type}): {evidence}"
                        characters.append(char_info)
            
            return "\n".join(characters) if characters else "æ— è§’è‰²ä¿¡æ¯"
            
        except Exception as e:
            log.info(f"âš ï¸ ä»CSVåŠ è½½è§’è‰²æ‘˜è¦å¤±è´¥: {e}")
            return "æ— è§’è‰²ä¿¡æ¯"

    def _load_character_summary_from_global(self, global_file: str, episode_id: str) -> str:
        """ä»å…¨å±€æ–‡ä»¶åŠ è½½è§’è‰²æ‘˜è¦"""
        try:
            with open(global_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æŸ¥æ‰¾å½“é›†çš„è§’è‰²ä¿¡æ¯
            per_episode = data.get('per_episode', [])
            for ep in per_episode:
                if ep.get('episode_id') == episode_id:
                    protagonists = ep.get('protagonists', [])
                    supporting = ep.get('supporting', [])
                    
                    summary_parts = []
                    for char in protagonists + supporting:
                        name = char.get('canonical_name', '')
                        traits = char.get('global_traits', '')
                        if name:
                            summary_parts.append(f"- {name}: {traits}")
                    
                    return "\n".join(summary_parts) if summary_parts else "æ— è§’è‰²ä¿¡æ¯"
            
            return "æ— è§’è‰²ä¿¡æ¯"
            
        except Exception as e:
            log.info(f"âš ï¸ ä»å…¨å±€æ–‡ä»¶åŠ è½½è§’è‰²æ‘˜è¦å¤±è´¥: {e}")
            return "æ— è§’è‰²ä¿¡æ¯"

    def _get_video_uri(self, episode_id: str) -> str:
        """è·å–è§†é¢‘URI"""
        try:
            # å°è¯•å¤šä¸ªè·¯å¾„
            paths = [
                f"{self.config.project_root}/{episode_id}/gcs_path.txt",
                f"{self.config.project_root}/{episode_id}/0_gcs_path.txt",
                f"{self.config.output_dir}/{episode_id}/0_gcs_path.txt"
            ]
            
            for path in paths:
                if os.path.exists(path):
                    return self.utils.load_text_file(path, "").strip()
            
            return self.utils.get_video_uri(episode_id, self.config.project_root)
            
        except Exception as e:
            log.info(f"âš ï¸ è·å–è§†é¢‘URIå¤±è´¥: {e}")
            return ""

    def _save_results(self, result: Dict[str, Any], turns_file: str, 
                     summary_file: str, episode_id: str):
        """ä¿å­˜ç»“æœ"""
        try:
            # ä¿å­˜JSONç»“æœ
            with open(turns_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            summary = self._generate_analysis_summary(result, episode_id)
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary)
                
        except Exception as e:
            log.info(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def _generate_analysis_summary(self, result: Dict[str, Any], episode_id: str) -> str:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        turns = result.get('dialogue_turns', [])
        metadata = result.get('metadata', {})
        
        summary = f"""# {episode_id} å¯¹è¯åˆ†ææ‘˜è¦

## å¤„ç†ç»Ÿè®¡
- **æ€»å¯¹è¯è½®æ¬¡**: {metadata.get('total_turns', len(turns))}
- **çº é”™æ¬¡æ•°**: {metadata.get('correction_count', 0)}
- **é‡æ„æ¬¡æ•°**: {metadata.get('reconstruction_count', 0)}
- **å¤„ç†æ¨¡å¼**: {metadata.get('processing_mode', 'unknown')}

## è§’è‰²å¯¹è¯ç»Ÿè®¡
"""
        
        # ç»Ÿè®¡æ¯ä¸ªè§’è‰²çš„å¯¹è¯æ¬¡æ•°
        speaker_stats = {}
        for turn in turns:
            speaker = turn.get('speaker', 'UNKNOWN')
            speaker_stats[speaker] = speaker_stats.get(speaker, 0) + 1
        
        for speaker, count in sorted(speaker_stats.items()):
            summary += f"- **{speaker}**: {count} è½®å¯¹è¯\n"
        
        summary += "\n## æƒ…æ„Ÿåˆ†æç»Ÿè®¡\n"
        
        # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
        emotion_stats = {}
        for turn in turns:
            emotion = turn.get('emotion', 'æœªçŸ¥')
            emotion_stats[emotion] = emotion_stats.get(emotion, 0) + 1
        
        for emotion, count in sorted(emotion_stats.items()):
            summary += f"- **{emotion}**: {count} æ¬¡\n"
        
        summary += "\n## å¯¹è¯è½®æ¬¡è¯¦æƒ…\n"
        
        # æ˜¾ç¤ºå…¨éƒ¨å¯¹è¯è½®æ¬¡
        for i, turn in enumerate(turns):
            summary += f"""
### è½®æ¬¡ {turn.get('turn_id', i+1)}
- **è¯´è¯äºº**: {turn.get('speaker', 'UNKNOWN')}
- **æƒ…æ„Ÿ**: {turn.get('emotion', 'æœªçŸ¥')}
- **æ„å›¾**: {turn.get('intent', 'æœªçŸ¥')}
- **å¯¹è¯å†…å®¹**: {turn.get('full_dialogue', '')}
- **åŸå§‹è¡Œå·**: {turn.get('original_indices', [])}
"""
        
        if len(turns) > 5:
            summary += f"\n... è¿˜æœ‰ {len(turns) - 5} ä¸ªå¯¹è¯è½®æ¬¡\n"
        
        return summary

    def _run_all_episodes(self) -> Dict[str, Any]:
        """å¤„ç†æ‰€æœ‰å‰§é›†"""
        episodes = self.utils.get_episode_list(self.config.project_root)
        results = []
        log.info(f"å¼€å§‹Step0.5: å¤„ç† {len(episodes)} ä¸ªå‰§é›†...")
        
        # è·å–å¹¶è¡Œé…ç½®ï¼šæ­¥éª¤çº§ -> å…¨å±€ -> é»˜è®¤ 3
        step_conf = self.config.get_step_config(5) or {}
        max_workers = step_conf.get('max_workers')
        if max_workers is None:
            conc_conf = getattr(self.config, 'concurrency', {}) or {}
            max_workers = conc_conf.get('max_workers', 3)
        try:
            max_workers = int(max_workers)
        except Exception:
            max_workers = 3
        if max_workers < 1:
            max_workers = 1
        log.info(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œçº¿ç¨‹å¤„ç†...")
        
        # ä½¿ç”¨å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_episode = {
                executor.submit(self._run_single_episode, ep): ep 
                for ep in episodes
            }
            
            # æ”¶é›†ç»“æœ
            for future in tqdm(as_completed(future_to_episode), total=len(episodes), desc="Step0.5"):
                episode_id = future_to_episode[future]
                try:
                    result = future.result()
                    result["episode_id"] = episode_id
                    results.append(result)
                    
                    # å®æ—¶æ˜¾ç¤ºå¤„ç†ç»“æœ
                    if result.get("status") == "success":
                        log.info(f"âœ… {episode_id}: {result.get('turns_count', 0)} è½®å¯¹è¯, "
                              f"çº é”™ {result.get('correction_count', 0)} æ¬¡, "
                              f"é‡æ„ {result.get('reconstruction_count', 0)} æ¬¡")
                    elif result.get("status") == "already_exists":
                        log.info(f"â­ï¸ {episode_id}: å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
                    else:
                        log.info(f"âŒ {episode_id}: å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    log.info(f"âŒ Step0.5 å¤„ç† {episode_id} å¤±è´¥: {e}")
                    results.append({
                        "episode_id": episode_id, 
                        "status": "failed", 
                        "error": str(e)
                    })
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = self._generate_statistics(results)
        log.info("\nğŸ“Š Step0.5 å¤„ç†å®Œæˆç»Ÿè®¡:")
        log.info(f"   æ€»å‰§é›†æ•°: {stats['total_episodes']}")
        log.info(f"   æˆåŠŸå¤„ç†: {stats['success_count']}")
        log.info(f"   å¤±è´¥å¤„ç†: {stats['failed_count']}")
        log.info(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        log.info(f"   æ€»å¯¹è¯è½®æ¬¡: {stats['total_turns']}")
        log.info(f"   å¹³å‡å¯¹è¯è½®æ¬¡: {stats['avg_turns']:.1f}")
        log.info(f"   æ€»çº é”™æ¬¡æ•°: {stats['total_corrections']}")
        log.info(f"   æ€»é‡æ„æ¬¡æ•°: {stats['total_reconstructions']}")
        log.info(f"   æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']} ç§’")
        log.info(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']} ç§’/episode")
        log.info(f"   å¹¶è¡Œçº¿ç¨‹æ•°: {max_workers}")
        
        return {
            "status": "completed", 
            "results": results,
            "statistics": stats
        }

    def _generate_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        total_episodes = len(results)
        success_results = [r for r in results if r.get("status") == "success"]
        failed_results = [r for r in results if r.get("status") == "failed"]
        
        success_count = len(success_results)
        failed_count = len(failed_results)
        success_rate = (success_count / total_episodes * 100) if total_episodes > 0 else 0
        
        total_turns = sum(r.get("turns_count", 0) for r in success_results)
        avg_turns = (total_turns / success_count) if success_count > 0 else 0
        
        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        processing_times = [r.get("processing_time", 0) for r in success_results if r.get("processing_time")]
        total_time = sum(processing_times)
        avg_time = (total_time / len(processing_times)) if processing_times else 0
        
        # çº é”™å’Œé‡æ„ç»Ÿè®¡
        total_corrections = sum(r.get("correction_count", 0) for r in success_results)
        total_reconstructions = sum(r.get("reconstruction_count", 0) for r in success_results)
        
        return {
            "total_episodes": total_episodes,
            "success_count": success_count,
            "failed_count": failed_count,
            "success_rate": success_rate,
            "total_turns": total_turns,
            "avg_turns": avg_turns,
            "total_processing_time": round(total_time, 2),
            "avg_processing_time": round(avg_time, 2),
            "total_corrections": total_corrections,
            "total_reconstructions": total_reconstructions,
            "failed_episodes": [r.get("episode_id", "unknown") for r in failed_results]
        }
